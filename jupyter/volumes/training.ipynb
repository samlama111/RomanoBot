{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "\n",
    "client = Config(path=\"./config/.hdfscli.cfg\").get_client(\n",
    "    \"dev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert files, taken from populate_hdfs\n",
    "files_to_upload = [\"transfers.csv\",\"competitions.csv\", \"appearances.csv\", \"clubs.csv\", \"games.csv\", \"players.csv\"]\n",
    "\n",
    "remote_path = \"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure remote path exists\n",
    "client.makedirs(remote_path)\n",
    "\n",
    "# Insert files, taken from populate_hdfs\n",
    "\n",
    "# Check if the file exists\n",
    "for file in files_to_upload:\n",
    "    local_path = f\"./data/{file}\"\n",
    "    print(f\"Checking if {file} exists in {remote_path}...\")\n",
    "    if client.status(remote_path + file, strict=False):\n",
    "        print(f\"{file} exists in {remote_path}!\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{file} does not exist in {remote_path}!\")\n",
    "    print(f\"Uploading {file} to {remote_path}...\")\n",
    "    # Upload a file to tmp, to be processed further\n",
    "    client.upload(remote_path, local_path)\n",
    "\n",
    "print(f\"contents in {remote_path}: \", client.list(\"/data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Dependant - `to_club_id`\n",
    "\n",
    "We are creating a classifier model, that would classify based on the independent variables below which club is the most likely for a future transfer.\n",
    "\n",
    "When using as a service, it'd be nice if `player_id` and `to_club_name` were only necessary inputs and the rest read from HDFS/other data storage.\n",
    "Let's presume that in these scenarios, the `transfer_season` would be the current one (24/25)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "1. Remove entries where `transfer_fee == NaN`, since these entries are usually internal transfers (or from lower league youth teams).\n",
    "2. Filter entries where `market_value_in_eur == Nan`, since we assume it's hard to find any info about these players\n",
    "\n",
    "For now, we already execute/apply the filtering. But in the future, we will do all the processing first and then train our model on the batches, (hopefully) never applying `take_all`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Potential additional steps)\n",
    "\n",
    "3. Remove retired players\n",
    "4. Drop `transfer_date` column, as we don't need it for anything (the `transfer_season` should be enough for everything time-related).\n",
    "5. Drop one of `from_club_name` or `from_club_id` (and the same for `to_club_...`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tables\n",
    "\n",
    "Other useful tables and their attributes:\n",
    "\n",
    "appearances.csv - minutes played, goals, assists\n",
    "(Would be hard to map to individual players playing, e.g. how do we know who was on the pitch when a goal was scored or conceded?) \n",
    "\n",
    "club_games.csv - own_position, opponent_goals, opponent_position\n",
    "\n",
    "clubs.csv - domestic_competition_id, squad_size, average_age, foreigners_percentage, national_team_players, net_transfer_record, (maybe to filter outdated clubs) last_season\n",
    "\n",
    "(IMO useless) competitions.csv\n",
    "game_events.csv - player_id, type (goal, assist, card)\n",
    "\n",
    "(To know no. of games started) game_lineups.csv - player_id, position, type (substitute, starter)\n",
    "\n",
    "(IMO useless) games.csv\n",
    "\n",
    "(Useful for training, to know the valuation at the time of transfer, maybe 1 year prior?) player_valuations.csv - date, market_value_in_eur, current_club_id, player_id\n",
    "\n",
    "players.csv - last_season (filter retired players), country_of_birth, country_of_citizenship, position, sub_position, foot, height_in_cm, contract_expiration_date, agent_name, market_value_in_eur, highest_market_value_in_eur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing club names/ids the best way possible:\n",
    "- initially as IDs, but that could be interpreted as ordinality by the model\n",
    "- ideally as embeddings - either of the club name or combinations such as \"club country + league + club name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ray (Unused ATM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(dashboard_host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "\n",
    "# Helper function to read CSV files from HDFS in chunks\n",
    "def read_csv_from_hdfs(client, file_path):\n",
    "    with client.read(file_path) as reader:\n",
    "        file_contents = reader.read()\n",
    "    \n",
    "    # Use pyarrow to read the CSV data from memory\n",
    "    table = csv.read_csv(pa.py_buffer(file_contents))\n",
    "    \n",
    "    # Convert the pyarrow Table to a Ray Dataset\n",
    "    return ray.data.from_arrow(table)\n",
    "\n",
    "transfers_ds = read_csv_from_hdfs(client, \"/data/transfers.csv\")\n",
    "clubs_ds = read_csv_from_hdfs(client, \"/data/clubs.csv\")\n",
    "competitions_ds = read_csv_from_hdfs(client, \"/data/competitions.csv\")\n",
    "players_ds = read_csv_from_hdfs(client, \"/data/players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with null values in 'transfer_fee' and 'market_value_in_eur'\n",
    "def filter_transfers(batch):\n",
    "    return batch[batch['transfer_fee'].notna() & batch['market_value_in_eur'].notna()]\n",
    "\n",
    "transfers_ds = transfers_ds.map_batches(filter_transfers, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_transfers_clubs(transfers_batch):\n",
    "    transfers_df = pd.DataFrame(transfers_batch)\n",
    "    clubs_df = clubs_ds.select_columns(['club_id', 'domestic_competition_id']).to_pandas()\n",
    "    \n",
    "    merged = transfers_df.merge(clubs_df, left_on='from_club_id', right_on='club_id', how='left', suffixes=('', '_from'))\n",
    "    merged = merged.rename(columns={'domestic_competition_id': 'from_competition_id'})\n",
    "    \n",
    "    merged = merged.merge(clubs_df, left_on='to_club_id', right_on='club_id', how='left', suffixes=('', '_to'))\n",
    "    merged = merged.rename(columns={'domestic_competition_id': 'to_competition_id'})\n",
    "    \n",
    "    return merged.drop(columns=['club_id', 'club_id_to', 'transfer_date'])\n",
    "\n",
    "transfers_ds = transfers_ds.map_batches(join_transfers_clubs, batch_format=\"pandas\")\n",
    "\n",
    "# Filter out null competition IDs\n",
    "transfers_ds = transfers_ds.filter(lambda row: row['from_competition_id'] is not None and row['to_competition_id'] is not None)\n",
    "\n",
    "def join_transfers_competitions(transfers_batch):\n",
    "    transfers_df = pd.DataFrame(transfers_batch)\n",
    "    competitions_df = competitions_ds.select_columns(['competition_id', 'country_name', 'sub_type']).to_pandas()\n",
    "    \n",
    "    merged = transfers_df.merge(competitions_df, left_on='from_competition_id', right_on='competition_id', how='left', suffixes=('', '_from'))\n",
    "    merged = merged.rename(columns={'country_name': 'from_country_name', 'sub_type': 'from_sub_type'})\n",
    "    \n",
    "    merged = merged.merge(competitions_df, left_on='to_competition_id', right_on='competition_id', how='left', suffixes=('', '_to'))\n",
    "    merged = merged.rename(columns={'country_name': 'to_country_name', 'sub_type': 'to_sub_type'})\n",
    "    \n",
    "    return merged.drop(columns=['competition_id', 'competition_id_to'])\n",
    "\n",
    "transfers_ds = transfers_ds.map_batches(join_transfers_competitions, batch_format=\"pandas\")\n",
    "\n",
    "\n",
    "# Join players_ds to transfers_ds\n",
    "def join_transfers_players(transfers_batch):\n",
    "    transfers_df = pd.DataFrame(transfers_batch)\n",
    "    players_df = players_ds.select_columns(['player_id', 'last_season', 'country_of_citizenship', 'position', 'sub_position', 'contract_expiration_date', 'highest_market_value_in_eur']).to_pandas()\n",
    "    \n",
    "    merged = transfers_df.merge(players_df, on='player_id', how='left')\n",
    "    return merged[merged['last_season'] > 2023]  # Filter 'retired' players\n",
    "\n",
    "transfers_ds = transfers_ds.map_batches(join_transfers_players, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transfers(batch):\n",
    "    df = batch.copy()\n",
    "    \n",
    "    # Replace transfer_season with transfer_season_num\n",
    "    df['transfer_season_end_year'] = df['transfer_season'].apply(lambda x: int(x.split('/')[0]) + 1)\n",
    "\n",
    "    # Replace countries with IDs\n",
    "    country_columns = ['from_country_name', 'to_country_name', 'country_of_citizenship']\n",
    "    all_countries = set()\n",
    "    for col in country_columns:\n",
    "        all_countries.update(df[col].dropna().unique())\n",
    "    country_id_mapping = {country: idx for idx, country in enumerate(sorted(all_countries))}\n",
    "\n",
    "    for col in country_columns:\n",
    "        df[f'{col}_id'] = df[col].map(country_id_mapping)\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    # Replace position with IDs\n",
    "    all_positions = df['position'].dropna().unique()\n",
    "    position_mapping = {position: idx for idx, position in enumerate(sorted(all_positions))}\n",
    "    df['position_id'] = df['position'].map(position_mapping)\n",
    "    df = df.drop(columns=['position'])\n",
    "\n",
    "    # Replace sub_position with IDs\n",
    "    all_sub_positions = df['sub_position'].dropna().unique()\n",
    "    sub_position_mapping = {sub_position: idx for idx, sub_position in enumerate(sorted(all_sub_positions))}\n",
    "    df['sub_position_id'] = df['sub_position'].map(sub_position_mapping)\n",
    "    df = df.drop(columns=['sub_position'])\n",
    "\n",
    "    # Convert contract_expiration_date\n",
    "    df['contract_expiration_date'] = pd.to_datetime(df['contract_expiration_date'], errors='coerce')\n",
    "    df['contract_expiration_date'] = df['contract_expiration_date'].dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "prepared_transfers_ds = transfers_ds.map_batches(prepare_transfers, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = ['player_id', 'from_club_id', 'market_value_in_eur', \n",
    "                  'transfer_season_end_year', 'from_country_name_id', \n",
    "                  'country_of_citizenship_id', 'position_id', 'sub_position_id', \n",
    "                  'contract_expiration_date', 'highest_market_value_in_eur']\n",
    "\n",
    "X = prepared_transfers_ds.select_columns(train_features)\n",
    "y = prepared_transfers_ds.select_columns(['to_club_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.train import ScalingConfig\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a function to prepare data and train the model\n",
    "def train_func(config):\n",
    "    # Get the Ray datasets\n",
    "    X = ray.get(config[\"X\"])\n",
    "    y = ray.get(config[\"y\"])\n",
    "    \n",
    "    # Convert to pandas (this will happen in parallel across workers)\n",
    "    X_pd = X.to_pandas()\n",
    "    y_pd = y.to_pandas()\n",
    "    \n",
    "    # Train the model\n",
    "    clf = RandomForestClassifier(n_estimators=config[\"n_estimators\"], random_state=42)\n",
    "    clf.fit(X_pd, y_pd.values.ravel())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    y_pred = clf.predict(X_pd)\n",
    "    accuracy = accuracy_score(y_pd, y_pred)\n",
    "    \n",
    "    # Report results\n",
    "    tune.report(accuracy=accuracy, model=clf)\n",
    "\n",
    "# Define the search space\n",
    "config = {\n",
    "    \"n_estimators\": tune.choice([50, 100, 200]),\n",
    "    \"X\": ray.put(X),  # Your Ray Dataset for features\n",
    "    \"y\": ray.put(y)   # Your Ray Dataset for labels\n",
    "}\n",
    "\n",
    "# Create the tuner\n",
    "tuner = tune.Tuner(\n",
    "    train_func,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(num_samples=1),  # Increase for hyperparameter tuning\n",
    "    run_config=ray.train.RunConfig()\n",
    ")\n",
    "\n",
    "# Run the tuning\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get the best result\n",
    "best_result = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n",
    "print(best_result)\n",
    "best_model = best_result.checkpoint.to_dict()[\"model\"]\n",
    "\n",
    "# Now you can use best_model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "print(transfers_ds.take(5))\n",
    "\n",
    "# If you need the final result as a Pandas DataFrame:\n",
    "# final_df = transfers_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(dashboard_host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "def read_csv_with_modin(client, hdfs_path):\n",
    "    with client.read(hdfs_path) as reader:\n",
    "        file_contents = reader.read()\n",
    "    \n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:\n",
    "        temp_file.write(file_contents)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    # Read the CSV file using Modin\n",
    "    df = pd.read_csv(temp_file_path)\n",
    "    \n",
    "    # Delete the temporary file\n",
    "    os.unlink(temp_file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "transfers_df = read_csv_with_modin(client, \"/data/transfers.csv\")\n",
    "clubs_df = read_csv_with_modin(client, \"/data/clubs.csv\")\n",
    "competitions_df = read_csv_with_modin(client, \"/data/competitions.csv\")\n",
    "players_df = read_csv_with_modin(client, \"/data/players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transfers\n",
    "# 1. Remove entries where `transfer_fee == NaN`, since these entries are usually internal transfers (or from lower league youth teams).\n",
    "# 2. Filter entries where `market_value_in_eur == Nan`, since we assume it's hard to find any info about these players\n",
    "\n",
    "transfers_df = transfers_df[transfers_df['transfer_fee'].notna()]\n",
    "transfers_df = transfers_df[transfers_df['market_value_in_eur'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tables\n",
    "transfers_df = transfers_df.merge(clubs_df[['club_id', 'domestic_competition_id']], left_on='from_club_id', right_on='club_id', how='left', validate='m:m')\n",
    "transfers_df = transfers_df.rename(columns={'domestic_competition_id': 'from_competition_id'})\n",
    "\n",
    "transfers_df = transfers_df.merge(clubs_df[['club_id', 'domestic_competition_id']], left_on='to_club_id', right_on='club_id', how='left', validate='m:m')\n",
    "transfers_df = transfers_df.rename(columns={'domestic_competition_id': 'to_competition_id'})\n",
    "\n",
    "transfers_df = transfers_df.drop(columns=['club_id_x', 'club_id_y', 'transfer_date'])\n",
    "transfers_df = transfers_df.dropna(subset=['from_competition_id', 'to_competition_id'])\n",
    "\n",
    "transfers_df = transfers_df.merge(competitions_df[['competition_id', 'country_name', 'sub_type']], left_on='from_competition_id', right_on='competition_id', how='left', validate='m:m')\n",
    "transfers_df = transfers_df.rename(columns={'country_name': 'from_country_name', 'sub_type': 'from_sub_type'})\n",
    "\n",
    "transfers_df = transfers_df.merge(competitions_df[['competition_id','country_name', 'sub_type']], left_on='to_competition_id', right_on='competition_id', how='left', validate='m:m')\n",
    "transfers_df = transfers_df.rename(columns={'country_name': 'to_country_name', 'sub_type': 'to_sub_type'})\n",
    "\n",
    "transfers_df = transfers_df.drop(columns=['competition_id_x', 'competition_id_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join players_df to transfers_df (columns last_season, country_of_birth, position, sub_position, contract_expiration_date, highest_market_value_in_eur)\n",
    "# using the player_id\n",
    "transfers_df = transfers_df.merge(players_df[['player_id', 'last_season', 'country_of_citizenship', 'position', 'sub_position', 'contract_expiration_date', 'highest_market_value_in_eur']], on='player_id', how='left', validate='m:m')\n",
    "\n",
    "# Filter 'retired' players \n",
    "transfers_df = transfers_df[transfers_df['last_season'] > 2023]\n",
    "\n",
    "transfers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_transfers_df = transfers_df.copy()\n",
    "# Replace transfer_season with transfer_season_num\n",
    "prepared_transfers_df['transfer_season_end_year'] = prepared_transfers_df['transfer_season'].apply(lambda x: int(x.split('/')[0]) + 1)\n",
    "\n",
    "# Replace countries with IDs for from_country_name, to_country_name, country_of_citizenship columns\n",
    "def get_country_id_mapping(df, columns):\n",
    "    all_countries = set()\n",
    "    for col in columns:\n",
    "        all_countries.update(df[col].unique())\n",
    "    return {country: idx for idx, country in enumerate(sorted(all_countries))}\n",
    "\n",
    "country_columns = ['from_country_name', 'to_country_name', 'country_of_citizenship']\n",
    "country_id_mapping = get_country_id_mapping(prepared_transfers_df, country_columns)\n",
    "\n",
    "for col in country_columns:\n",
    "    prepared_transfers_df[f'{col}_id'] = prepared_transfers_df[col].map(country_id_mapping)\n",
    "    prepared_transfers_df = prepared_transfers_df.drop(columns=[col])\n",
    "\n",
    "# Replace position and sub_position respectively using the same approach\n",
    "def get_position_id_mapping(df):\n",
    "    all_positions = df['position'].unique()\n",
    "    return {position: idx for idx, position in enumerate(sorted(all_positions))}\n",
    "\n",
    "position_mapping = get_position_id_mapping(prepared_transfers_df)\n",
    "\n",
    "prepared_transfers_df['position_id'] = prepared_transfers_df['position'].map(position_mapping)\n",
    "prepared_transfers_df = prepared_transfers_df.drop(columns=['position'])\n",
    "\n",
    "def get_sub_position_id_mapping(df):\n",
    "    all_sub_positions = df['sub_position'].unique()\n",
    "    return {sub_position: idx for idx, sub_position in enumerate(sorted(all_sub_positions))}\n",
    "\n",
    "sub_position_mapping = get_sub_position_id_mapping(prepared_transfers_df)\n",
    "\n",
    "prepared_transfers_df['sub_position_id'] = prepared_transfers_df['sub_position'].map(sub_position_mapping)\n",
    "prepared_transfers_df = prepared_transfers_df.drop(columns=['sub_position'])\n",
    "\n",
    "# Convert contract_expiration_date\n",
    "prepared_transfers_df['contract_expiration_date'] = pd.to_datetime(prepared_transfers_df['contract_expiration_date'])\n",
    "prepared_transfers_df['contract_expiration_date'] = prepared_transfers_df['contract_expiration_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = ['player_id', 'from_club_id', 'market_value_in_eur', \n",
    "                  'transfer_season_end_year', 'from_country_name_id', \n",
    "                  'country_of_citizenship_id', 'position_id', 'sub_position_id', \n",
    "                  'contract_expiration_date', 'highest_market_value_in_eur']\n",
    "\n",
    "X = prepared_transfers_df[train_features]\n",
    "y = prepared_transfers_df['to_club_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame({'player_id': X_test['player_id'], 'predicted_club_id': y_pred})\n",
    "\n",
    "# Join y_pred_df with players_df on player_id\n",
    "y_pred_df = y_pred_df.merge(players_df[['player_id', 'name']], on='player_id', how='left')\n",
    "\n",
    "# Join y_pred_df with clubs_df on predicted_club_id\n",
    "y_pred_df = y_pred_df.merge(clubs_df[['club_id', 'name']], left_on='predicted_club_id', right_on='club_id', how='left')\n",
    "\n",
    "# Rename columns for clarity\n",
    "y_pred_df = y_pred_df.rename(columns={'name_x': 'player_name', 'name_y': 'predicted_club_name'})\n",
    "\n",
    "# Print y_pred_df\n",
    "print(y_pred_df[['player_id', 'player_name', 'predicted_club_id', 'predicted_club_name']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_transfer_probability(player_data, target_club_id):\n",
    "    # Ensure player_data has all necessary features\n",
    "    for feature in train_features:\n",
    "        if feature not in player_data:\n",
    "            raise ValueError(f\"Missing feature: {feature}\")\n",
    "    \n",
    "    # Create a 2D array with a single sample\n",
    "    input_data = np.array([player_data[feature] for feature in train_features]).reshape(1, -1)\n",
    "    \n",
    "    # Get probabilities for all classes\n",
    "    probabilities = clf.predict_proba(input_data)[0]\n",
    "    \n",
    "    # Find the index of the target club ID in the classes\n",
    "    target_index = np.where(clf.classes_ == target_club_id)[0]\n",
    "    \n",
    "    # Return the probability for the target club\n",
    "    if len(target_index) > 0:\n",
    "        return probabilities[target_index[0]]\n",
    "    else:\n",
    "        return 0.0  # Return 0 if the club ID is not in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Czech player\n",
    "example_player = X[X['player_id'] == 195778].loc[0]\n",
    "\n",
    "target_club_ids = [27, 31, 40, 984]  # Example club IDs\n",
    "\n",
    "for club_id in target_club_ids:\n",
    "    probability = predict_transfer_probability(example_player, club_id)\n",
    "    club_name = clubs_df[clubs_df['club_id'] == club_id]['name'].values[0]\n",
    "    print(f\"Probability of transfer to {club_name} (ID: {club_id}): {probability:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray\n",
    "# ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
