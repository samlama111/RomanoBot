{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "\n",
    "client = Config(path=\"./config/.hdfscli.cfg\").get_client(\n",
    "    \"dev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert files, taken from populate_hdfs\n",
    "import os\n",
    "files_to_upload = os.listdir('data/') \n",
    "\n",
    "remote_path = \"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure remote path exists\n",
    "client.makedirs(remote_path)\n",
    "\n",
    "# Insert files, taken from populate_hdfs\n",
    "\n",
    "# Check if the file exists\n",
    "for file in files_to_upload:\n",
    "    local_path = f\"./data/{file}\"\n",
    "    print(f\"Checking if {file} exists in {remote_path}...\")\n",
    "    if client.status(remote_path + file, strict=False):\n",
    "        print(f\"{file} exists in {remote_path}!\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{file} does not exist in {remote_path}!\")\n",
    "    print(f\"Uploading {file} to {remote_path}...\")\n",
    "    # Upload a file to tmp, to be processed further\n",
    "    client.upload(remote_path, local_path)\n",
    "\n",
    "print(f\"contents in {remote_path}: \", client.list(\"/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(dashboard_host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file from HDFS\n",
    "with client.read(f\"{remote_path}transfers.csv\") as reader:\n",
    "    file_contents = reader.read().decode('utf-8')\n",
    "\n",
    "# Load the CSV data into a Pandas DataFrame\n",
    "from io import StringIO\n",
    "df = pd.read_csv(StringIO(file_contents))\n",
    "\n",
    "# Convert the Pandas DataFrame into a Ray Dataset\n",
    "dataset = ray.data.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "1. Remove entries where `transfer_fee == NaN`, since these entries are usually internal transfers (or from lower league youth teams).\n",
    "2. Filter entries where `market_value_in_eur == Nan`, since we assume it's hard to find any info about these players\n",
    "\n",
    "For now, we already execute/apply the filtering. But in the future, we will do all the processing first and then train our model on the batches, (hopefully) never applying `take_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a simple filter function\n",
    "def filter_func(batch):\n",
    "    return batch[\n",
    "        batch['transfer_fee'].notna() & \n",
    "        (~batch['transfer_fee'].isna()) & \n",
    "        batch['market_value_in_eur'].notna() & \n",
    "        (~batch['market_value_in_eur'].isna())\n",
    "    ]\n",
    "\n",
    "# Apply the filter using map_batches for better parallelization\n",
    "start_time = time.time()\n",
    "filtered_ds = dataset.map_batches(\n",
    "    filter_func,\n",
    "    batch_format=\"pandas\",\n",
    "    num_cpus=1  # This will allow up to 8 batches to be processed in parallel\n",
    ")\n",
    "\n",
    "# Materialize the results\n",
    "result = filtered_ds.take_all()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Filtering time: {end_time - start_time} seconds\")\n",
    "print(f\"Original dataset size: {dataset.count()}\")\n",
    "print(f\"Filtered dataset size: {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Potential additional steps)\n",
    "\n",
    "3. Remove retired players\n",
    "4. Drop `transfer_date` column, as we don't need it for anything (the `transfer_season` should be enough for everything time-related).\n",
    "5. Drop one of `from_club_name` or `from_club_id` (and the same for `to_club_...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this table\n",
    "# player_id, transfer_season, from_club_id, to_club_id, market_value_in_eur, fee\n",
    "\n",
    "# Other useful tables and their attributes\n",
    "\n",
    "# appearances.csv - minutes played, goals, assists\n",
    "# (Would be hard to map to individual players playing, e.g. how do we know who was on the pitch when a goal was scored or conceded?) \n",
    "# club_games.csv - own_position, opponent_goals, opponent_position\n",
    "# clubs.csv - domestic_competition_id, squad_size, average_age, foreigners_percentage, national_team_players, net_transfer_record, (maybe to filter outdated clubs) last_season\n",
    "# (IMO useless) competitions.csv\n",
    "# game_events.csv - player_id, type (goal, assist, card)\n",
    "# (To know no. of games started) game_lineups.csv - player_id, position, type (substitute, starter)\n",
    "# (IMO useless) games.csv\n",
    "# (Useful for training, to know the valuation at the time of transfer, maybe 1 year prior?) player_valuations.csv - date, market_value_in_eur, current_club_id, player_id\n",
    "# players.csv - last_season (filter retired players), country_of_birth, country_of_citizenship, position, sub_position, foot, height_in_cm, contract_expiration_date, agent_name, market_value_in_eur, highest_market_value_in_eur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Independent - player information (from other tables), `market_value_in_eur`, `from_club_name`/`from_club_id`\n",
    "\n",
    "Dependant - `to_club_id`/`to_club_name`, `transfer_fee`\n",
    "\n",
    "Because we have multiple dependant variables, there would be two models - one regression one predicting the transfer fee and another one (classifier most likely) predicting the club ID/name.\n",
    "\n",
    "When using as a service, it'd be nice if `player_id` and `to_club_name` were only necessary inputs and the rest read from HDFS/other data storage.\n",
    "Let's presume that in these scenarios, the `transfer_season` would be the current one (24/25)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing club names/ids the best way possible:\n",
    "- initially as IDs, but that could be interpreted as ordinality by the model\n",
    "- ideally as embeddings - either of the club name or combinations such as \"club country + league + club name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
